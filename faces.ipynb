{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO\n",
    "\n",
    "- Plot class imbalance plot\n",
    "- Neural network implementation\n",
    "- Cross validation\n",
    "- Third model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "face_dir = os.path.join(current_dir, \"cmu+face+images\", \"faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary based on face orientation\n",
    "data_dict = {\"left\": [],\n",
    "             \"right\": [],\n",
    "             \"straight\": [],\n",
    "             \"up\" : []}\n",
    "\n",
    "for human in os.listdir(face_dir):\n",
    "    if not human.startswith(\".\"):\n",
    "        for image in os.listdir(os.path.join(face_dir, human)):\n",
    "            if (not image.endswith(\".bad\")) and (not image.endswith(\"2.pgm\")) and (not image.endswith(\"4.pgm\")):\n",
    "                key = image.split(\"_\")[1]\n",
    "                data_dict[key].append(os.path.join(face_dir, human, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders with respective labels\n",
    "orient_folder = \"face_orientation\"\n",
    "\n",
    "location = os.path.join(current_dir, \"cmu+face+images\")\n",
    "if not os.path.exists(os.path.join(location, orient_folder)):\n",
    "    os.mkdir(os.path.join(location, orient_folder))\n",
    "face_orientation =  os.path.join(location, orient_folder)\n",
    "\n",
    "for key in data_dict.keys():\n",
    "    if not os.path.exists(os.path.join(face_orientation, key)):\n",
    "        os.mkdir(os.path.join(face_orientation, key))\n",
    "    path = os.path.join(face_orientation, key)\n",
    "    for image in data_dict[key]:\n",
    "        shutil.copy(image, os.path.join(path, os.path.split(image)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check example images\n",
    "current_dir = os.getcwd()\n",
    "orient_folder = \"face_orientation\"\n",
    "location = os.path.join(current_dir, \"cmu+face+images\")\n",
    "face_orientation =  os.path.join(location, orient_folder)\n",
    "\n",
    "# Gather all data\n",
    "keys = [\"left\", \"right\", \"straight\", \"up\"]\n",
    "all_data = [(Image.open(os.path.join(face_orientation, key, image)).convert(mode=\"L\"), key) for key in keys for image in os.listdir(os.path.join(face_orientation, key))]\n",
    "\n",
    "plt.figure()\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        plt.subplot2grid((4, 4), (row, col))\n",
    "        example = all_data[row*200 + col][0]\n",
    "        plt.imshow(example, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "plt.savefig(\"plots/samples.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[0][0], all_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([elem[0] for elem in all_data]) \n",
    "X = X.reshape((len(all_data), -1))/255.0 # (624, 120, 128) --> (624, 15360) and [0, 255] -> [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (120, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(data, first_k=90, shape=(120, 128), speak=True):\n",
    "\n",
    "    center = data.mean(axis= 0)\n",
    "    centered = data - center # Subtract mean \n",
    "\n",
    "    u, s, vt = np.linalg.svd(centered)\n",
    "    \n",
    "    lambdas = np.square(s)/(X.shape[0] - 1)\n",
    "\n",
    "    first_k = 90\n",
    "    eig_first_k = lambdas[0:first_k]/sum(lambdas)\n",
    "    preserved = [sum(eig_first_k[0:i+1]) for i in range(first_k)]\n",
    "    eig_idx = first_k - np.sum(np.array(preserved) >= 0.9)\n",
    "\n",
    "    if speak:\n",
    "        plt.figure()\n",
    "        plt.imshow(center.reshape(shape))\n",
    "        plt.axis(\"off\")\n",
    "        #plt.title(\"Mean of the data matrix\")\n",
    "        plt.savefig(\"plots/pca_mean.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(centered[0].reshape(shape))\n",
    "        plt.axis(\"off\")\n",
    "        #plt.title(\"Centered image example\")\n",
    "\n",
    "\n",
    "        # Plot the explained variance ratio\n",
    "        plt.figure()\n",
    "        plt.bar(np.linspace(1,first_k,first_k), eig_first_k, label=\"Variance\", color=\"black\")\n",
    "        plt.xticks(np.linspace(1,first_k,first_k), [x + 1 for x in range(first_k)])\n",
    "        plt.xticks(np.linspace(0, first_k, 10), [str(num) for num in np.linspace(0, first_k, 10, dtype=int)])\n",
    "        plt.yticks(np.arange(0.0, 1.0, 0.1), [str(num)[:3] for num in np.arange(0.0, 1.0, 0.1)])\n",
    "        plt.xlabel(\"kth largest principal component\")\n",
    "        plt.ylabel(\"Proportion of variance captured\")\n",
    "        #plt.title(f\"Proportion of variance captured by the first ten largest principal components \\n eig_idx = {eig_idx}\")\n",
    "        plt.plot(np.linspace(1,first_k,first_k), preserved, label=\"Cumulative Variance\", color=\"blue\")\n",
    "        plt.axhline(y=0.9, color=\"red\", label=\"90% Variance\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.savefig(\"plots/proportion_of_variance.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "        # Plot first 16 principal components\n",
    "        plt.figure()\n",
    "        plotdim = 4\n",
    "        #plt.title(f\"First {plotdim**2} principal components\")\n",
    "        for row in range(plotdim):\n",
    "            for col in range(plotdim):\n",
    "                plt.subplot2grid((plotdim, plotdim), (row, col))\n",
    "                plt.imshow(X=vt[plotdim*row + col].reshape(shape))\n",
    "                plt.axis(\"off\")\n",
    "        \n",
    "        plt.savefig(\"plots/pca_first_k.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "    return center, lambdas, vt, eig_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center, lambdas, vt, eig_idx = PCA(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coefficients and reconstruct image\n",
    "coeffs = X[0].reshape((1, -1)) @ vt[:eig_idx].T\n",
    "recon = coeffs @ vt[:eig_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0].reshape(shape))\n",
    "plt.axis(\"off\")\n",
    "#plt.title(\"First image\")\n",
    "plt.savefig(\"plots/first_img.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.imshow((recon + center).reshape(shape))\n",
    "plt.axis(\"off\")\n",
    "#plt.title(f\"First image reconstructed with first {eig_idx} components\")\n",
    "plt.savefig(\"plots/first_img_reconstruction.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNMF(data, shape=(120, 128), embedding_size=64, iterations=1000):\n",
    "\n",
    "    nnmf_scaler = np.sqrt(np.mean(data)/data.shape[0])\n",
    "    nnmf_loss = []\n",
    "\n",
    "    # Initialize non-negative matrices\n",
    "    W = abs(np.random.standard_normal((X.shape[0], embedding_size)))*nnmf_scaler\n",
    "    H = abs(np.random.standard_normal((embedding_size, X.shape[-1])))*nnmf_scaler\n",
    "\n",
    "    for iteration in tqdm(range(iterations)):\n",
    "        W_numerator = np.matmul(X, H.T)\n",
    "        W_denominator = np.matmul(np.matmul(W, H), H.T)\n",
    "        W_alpha = np.divide(W_numerator, W_denominator)\n",
    "        \n",
    "        W = W*W_alpha\n",
    "        \n",
    "        H_numerator = np.matmul(W.T, X)\n",
    "        H_denominator = np.matmul(np.matmul(W.T, W), H)\n",
    "        H_alpha = np.divide(H_numerator, H_denominator)\n",
    "        \n",
    "        H = H*H_alpha\n",
    "        \n",
    "        nnmf_loss.append(np.linalg.norm(X - np.matmul(W, H), \"fro\"))\n",
    "    \n",
    "    return W, H, nnmf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H, nnmf_loss = NNMF(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.grid()\n",
    "plt.plot(nnmf_loss)\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(\"plots/nmf_loss.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot NMF-faces\n",
    "plt.figure()\n",
    "plot_dim = 4\n",
    "for i in range(plot_dim):\n",
    "    for j in range(plot_dim):\n",
    "        plt.subplot(plot_dim, plot_dim, plot_dim*i + j + 1)\n",
    "        plt.imshow(H[plot_dim*i + j,:].reshape(shape))\n",
    "        plt.axis(\"off\")\n",
    "plt.savefig(\"plots/nmf_features.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain mixing coefficients for the first 6 people, NMF\n",
    "coeff_six_images_nmf = W[0,:].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the first 6 images, NMF\n",
    "recon_six_images_nmf = np.matmul(coeff_six_images_nmf, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 6 reconstructed faces, NMF\n",
    "plt.figure()\n",
    "plt.imshow(recon_six_images_nmf.reshape(shape))\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"plots/first_img_nmf_recon.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JL Transform (Random Projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jl_error = 0.5\n",
    "jl_bound = 24*np.log(X.shape[-1])/(3*jl_error**2 - 2*jl_error**3)\n",
    "print(f\"Minimum dimension for random projection {jl_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_proj_matrix = np.random.multivariate_normal(np.zeros(int(jl_bound)), np.eye(int(jl_bound))/jl_bound**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_proj_matrix = np.random.normal(loc=0, scale=1/jl_bound, size=(X.shape[-1], int(jl_bound)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_proj_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_proj_coeffs = X[0].reshape((1, -1)) @ random_proj_matrix\n",
    "random_proj_recon = random_proj_coeffs @ random_proj_matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_no_replacement(arr, num):\n",
    "    np.random.shuffle(arr)\n",
    "    x, arr = arr[:num], arr[num:]\n",
    "    return x, arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(data, labels, train_split=80):\n",
    "    assert(0 <= train_split <= 100)\n",
    "\n",
    "    train_data, train_labels = [], []\n",
    "    test_data, test_labels = [], []\n",
    "    targets = np.array([m[-1] for m in data])\n",
    "    for label in labels.keys():\n",
    "        points = np.array([data[i][0] for i in (targets == label).nonzero()[0]])\n",
    "        train, test = sample_no_replacement(points, len(points)*train_split//100)\n",
    "        train_data.append(train)\n",
    "        test_data.append(test)\n",
    "        for i in range(len(train)):\n",
    "            train_labels.append(label)\n",
    "        for i in range(len(test)):\n",
    "            test_labels.append(label)\n",
    "    \n",
    "    train_data = np.concatenate(train_data)\n",
    "    train_data = train_data.reshape(train_data.shape[0], -1)/255.0\n",
    "    train_labels = np.array([labels[elem] for elem in train_labels])\n",
    "    test_data = np.concatenate(test_data)\n",
    "    test_data = test_data.reshape(test_data.shape[0], -1)/255.0\n",
    "    test_labels = np.array([labels[elem] for elem in test_labels])\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\"left\": 0, \"right\": 1, \"straight\" : 2, \"up\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels = create_train_test(all_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, vt, eig_idx = PCA(train_data, speak=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test embeddings through PCA\n",
    "train_embeddings = train_data.reshape((train_data.shape[0], -1)) @ vt[:eig_idx].T\n",
    "test_embeddings = test_data.reshape((test_data.shape[0], -1)) @ vt[:eig_idx].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(train_embeddings, train_labels, test_embeddings, test_labels, k=3, speak=True):\n",
    "\n",
    "    assert(k > 0)\n",
    "\n",
    "    knn_confusion_matrix = np.zeros(shape=[len(labels)]*2)\n",
    "\n",
    "    for data, label in zip(test_embeddings, test_labels):\n",
    "\n",
    "        distances = np.linalg.norm(train_embeddings - data, axis=1)\n",
    "        neighbors = np.sort(distances)[:k]\n",
    "        preds = []\n",
    "\n",
    "        for neighbor in neighbors:\n",
    "            pred_idx = np.where(distances == neighbor)\n",
    "            preds.append(train_labels[pred_idx][0])\n",
    "        prediction = stats.mode(preds).mode\n",
    "        \n",
    "        knn_confusion_matrix[label, prediction] += 1\n",
    "\n",
    "    knn_accuracy = knn_confusion_matrix.trace()/knn_confusion_matrix.sum()\n",
    "\n",
    "    if speak:\n",
    "        print(f\"{k}-KNN accuracy: {knn_accuracy}\")\n",
    "\n",
    "        plt.figure()\n",
    "        sns.heatmap(knn_confusion_matrix, annot=True, cbar=False, xticklabels=labels.keys(), yticklabels=labels.keys(), cmap=\"YlGnBu\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Ground truth\")\n",
    "        #plt.title(f\"{k}-KNN confusion matrix\")\n",
    "        plt.savefig(f\"plots/knn_confusion_{k}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    \n",
    "    return knn_accuracy, knn_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16\n",
    "KNN(train_embeddings, train_labels, test_embeddings, test_labels, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from neural_network.layer import *\n",
    "from queue import Queue\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = np.ones((10, 1))\n",
    "y_target = np.array([[0.5], [0.5]])\n",
    "linear1 = Linear(10, 5)\n",
    "relu1 = ReLU(linear1)\n",
    "linear2 = Linear(5, 2, relu1)\n",
    "relu2 = ReLU(linear2)\n",
    "linear3 = Linear(2, 2, relu2)\n",
    "sigmoid = Sigmoid(linear3)\n",
    "loss_layer = MSE_Loss(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x_input, first_layer):\n",
    "    queue = Queue()\n",
    "    node = first_layer\n",
    "    node_input = x_input\n",
    "    node.forward(node_input)\n",
    "\n",
    "    for child in node.children:\n",
    "        queue.put(child)\n",
    "\n",
    "    while not queue.empty():\n",
    "        node = queue.get()\n",
    "         \n",
    "        node.forward(node_input)\n",
    "\n",
    "        for child in node.children:\n",
    "            if not issubclass(type(child), Loss):\n",
    "                queue.put(child)\n",
    "    \n",
    "    return node.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(loss_layer):\n",
    "    queue = Queue()\n",
    "    queue.put(loss_layer)\n",
    "\n",
    "    while not queue.empty():\n",
    "        node = queue.get()\n",
    "        # print(node)\n",
    "\n",
    "        node.backward()\n",
    "        node.update(0.01)\n",
    "        # update here\n",
    "        for parent in node.parents:\n",
    "            queue.put(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_array = []\n",
    "for i in range(10000):\n",
    "    out = forward_pass(x_input, linear1)\n",
    "    print(out)\n",
    "    loss = loss_layer.forward(out, y_target)\n",
    "    loss_array.append(loss)\n",
    "    backward_pass(loss_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = np.ones((10, 1))\n",
    "y_target = np.array([[0.5], [0.5]])\n",
    "linear1 = Linear(10, 5)\n",
    "relu1 = ReLU(linear1)\n",
    "linear2 = Linear(5, 2, relu1)\n",
    "relu2 = ReLU(linear2)\n",
    "linear3 = Linear(2, 2, relu2)\n",
    "addition1 = Addition([relu2, linear3])\n",
    "sigmoid = Sigmoid(addition1)\n",
    "loss_layer = MSE_Loss(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_array = []\n",
    "for i in range(1000):\n",
    "    out = forward_pass(x_input, linear1)\n",
    "    print(out)\n",
    "    loss = loss_layer.forward(out, y_target)\n",
    "    loss_array.append(loss)\n",
    "    backward_pass(loss_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = forward_pass(x_input, linear1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_layer.forward(out, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_pass(loss_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = np.array([[1.0], [0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_layer = MSE_Loss(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_layer.parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_layer.forward(out.out, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = Queue()\n",
    "queue.put(loss_layer)\n",
    "\n",
    "while not queue.empty():\n",
    "    node = queue.get()\n",
    "    print(node)\n",
    "\n",
    "    node.backward()\n",
    "    node.update(0.01)\n",
    "    # update here\n",
    "    for parent in node.parents:\n",
    "        queue.put(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = np.ones((10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = Linear(10, 2)\n",
    "out = Sigmoid(linear1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deneme = []\n",
    "deneme.append(out)\n",
    "parents = out.parents\n",
    "deneme.extend(parents)\n",
    "parents = [node.parents for node in parents]\n",
    "print(parents)\n",
    "\n",
    "while len(parents) > 0:\n",
    "\n",
    "    parents = [node.parents for node in parents]\n",
    "    deneme.extend(parents)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deneme"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
